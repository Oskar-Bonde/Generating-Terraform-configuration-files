/opt/conda/lib/python3.7/site-packages/torch/distributed/launch.py:164: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  "The module torch.distributed.launch is deprecated "
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : codeparrot_training.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 16
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29500
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_bmnbki_c/none_087jy0na
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3.7
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/utils/store.py:53: FutureWarning: This is an experimental API and will be changed in future.
  "This is an experimental API and will be changed in future.", FutureWarning
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
  role_ranks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
  global_ranks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
  role_world_sizes=[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]
  global_world_sizes=[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/2/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/3/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker4 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/4/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker5 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/5/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker6 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/6/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker7 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/7/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker8 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/8/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker9 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/9/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker10 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/10/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker11 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/11/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker12 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/12/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker13 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/13/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker14 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/14/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker15 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_0/15/error.json
04/29/2022 11:20:40 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 16
Process index: 0
Local process index: 0
Device: cuda:0
Mixed precision type: fp16

loading configuration file https://huggingface.co/gpt2-large/resolve/main/config.json from cache at /home/oskar_bonde_ericsson_com/.cache/huggingface/transformers/d82fb41558a2cc40bb6e10a57bbfbd9ff2f3c6614072f05afdfa8f44d566d2ba.55d263c4ba1f8b022997c21dfa03fb8933c57bc9c978354e0a62896cfd837a89
Model config GPT2Config {
  "_name_or_path": "gpt2-large",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.15.0",
  "use_cache": true,
  "vocab_size": 50257
}

loading weights file https://huggingface.co/gpt2-large/resolve/main/pytorch_model.bin from cache at /home/oskar_bonde_ericsson_com/.cache/huggingface/transformers/234578a5793e64713ba846b4c5e181e043f48b33140622e2c1dd623b665de3f9.4780ef91b17260f8dac8a3c2183aa338b27365326fb706e74db40b03749f8aba
All model checkpoint weights were used when initializing GPT2LMHeadModel.

All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
loading file https://huggingface.co/lvwerra/codeparrot/resolve/main/vocab.json from cache at /home/oskar_bonde_ericsson_com/.cache/huggingface/transformers/d4c2482a5da8882c8d1ec0e3221150100f233d2b2acf66878febcb6cc56f18de.e1e79be960ac552aa28d70494c56c5899ed90b1537d41fd037262657a9befd6a
loading file https://huggingface.co/lvwerra/codeparrot/resolve/main/merges.txt from cache at /home/oskar_bonde_ericsson_com/.cache/huggingface/transformers/de35c3c3c22ca9117b396a8474954092c4fffe984aa1267422c6c9af66298340.d722e5972b12fc42e29215757e41549636c5ce2c31cb49e3e46e35e10d103923
loading file https://huggingface.co/lvwerra/codeparrot/resolve/main/tokenizer.json from cache at /home/oskar_bonde_ericsson_com/.cache/huggingface/transformers/df053a83882dfbc6d41d077adeceea2f0848d9e13a2c915e2cb44fcb9f0d3307.e4fbee984d45ae8f3f9c9eb054628582f6489ad789c3552c94efb1208b03ffdf
loading file https://huggingface.co/lvwerra/codeparrot/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/lvwerra/codeparrot/resolve/main/special_tokens_map.json from cache at /home/oskar_bonde_ericsson_com/.cache/huggingface/transformers/c7bb2e7b7bb9c7fe62d1a881eec761303a488f6fbf706cf7f82033487c8af553.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22
loading file https://huggingface.co/lvwerra/codeparrot/resolve/main/tokenizer_config.json from cache at /home/oskar_bonde_ericsson_com/.cache/huggingface/transformers/f01f6d46c71625883cae6b83304b33a10d6a0c077e4ea7fc8639df22a017294a.173e36d458460b8910258ea66979b41056ea6ae08a7c74b11807859b62b51acc
04/29/2022 11:20:54 - WARNING - datasets.builder - Using custom data configuration train-b61d70ef90b56455
04/29/2022 11:20:54 - WARNING - datasets.builder - Using custom data configuration validation-bfbd664215916c2f
Start training
Start training
Start training
Start training
Start training
Start training
Start training
Start training
Start training
Start training
Start training
Start training
Start training
Start training
Start training
Start training
Token indices sequence length is longer than the specified maximum sequence length for this model (3254 > 1024). Running this sequence through the model will result in indexing errors
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
04/29/2022 11:21:30 - INFO - root - Reducer buckets have been rebuilt in this iteration.
Traceback (most recent call last):
Traceback (most recent call last):
  File "codeparrot_training.py", line 246, in <module>
Traceback (most recent call last):
  File "codeparrot_training.py", line 246, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "codeparrot_training.py", line 246, in <module>
  File "codeparrot_training.py", line 246, in <module>
  File "codeparrot_training.py", line 246, in <module>
Traceback (most recent call last):
  File "codeparrot_training.py", line 246, in <module>
Traceback (most recent call last):
  File "codeparrot_training.py", line 246, in <module>
Traceback (most recent call last):
  File "codeparrot_training.py", line 246, in <module>
    loss = model(batch, labels=batch, use_cache=False).loss
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
        loss = model(batch, labels=batch, use_cache=False).loss    loss = model(batch, labels=batch, use_cache=False).loss
    loss = model(batch, labels=batch, use_cache=False).loss
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
loss = model(batch, labels=batch, use_cache=False).loss
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl

  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    loss = model(batch, labels=batch, use_cache=False).loss
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    loss = model(batch, labels=batch, use_cache=False).loss
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    loss = model(batch, labels=batch, use_cache=False).loss
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
        return forward_call(*input, **kwargs)    return forward_call(*input, **kwargs)
return forward_call(*input, **kwargs)    

            return forward_call(*input, **kwargs)  File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__
      File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__
return forward_call(*input, **kwargs)  File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__
return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

return forward_call(*input, **kwargs)

  File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__

  File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__
  File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__
  File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__
  File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__
        return convert_to_fp32(self.model_forward(*args, **kwargs))return convert_to_fp32(self.model_forward(*args, **kwargs))

  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
                    return convert_to_fp32(self.model_forward(*args, **kwargs))return convert_to_fp32(self.model_forward(*args, **kwargs))  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
return convert_to_fp32(self.model_forward(*args, **kwargs))return convert_to_fp32(self.model_forward(*args, **kwargs))return convert_to_fp32(self.model_forward(*args, **kwargs))




  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
    return func(*args, **kwargs)    
    return func(*args, **kwargs)  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    return func(*args, **kwargs)
                return func(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
return func(*args, **kwargs)return func(*args, **kwargs)return func(*args, **kwargs)return func(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward




  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    output = self.module(*inputs[0], **kwargs[0])    
output = self.module(*inputs[0], **kwargs[0])  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
        
        output = self.module(*inputs[0], **kwargs[0])        output = self.module(*inputs[0], **kwargs[0])  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
output = self.module(*inputs[0], **kwargs[0])output = self.module(*inputs[0], **kwargs[0])
output = self.module(*inputs[0], **kwargs[0])output = self.module(*inputs[0], **kwargs[0])


  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl


  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward
    return forward_call(*input, **kwargs)
    return forward_call(*input, **kwargs)  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward

    return forward_call(*input, **kwargs)
    return forward_call(*input, **kwargs)  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward

              File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward
return forward_call(*input, **kwargs)  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward
return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)


  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward
    return_dict=return_dict,
      File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return_dict=return_dict,    return_dict=return_dict,    
      File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
return_dict=return_dict,
      File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return_dict=return_dict,return_dict=return_dict,
return_dict=return_dict,return_dict=return_dict,

  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl


  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward
    return forward_call(*input, **kwargs)
      File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward
return forward_call(*input, **kwargs)    
return forward_call(*input, **kwargs)      File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward

return forward_call(*input, **kwargs)  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward

  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward
    encoder_attention_mask,
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
    encoder_attention_mask,
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
    encoder_attention_mask,
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
    encoder_attention_mask,
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
    encoder_attention_mask,
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
        encoder_attention_mask,encoder_attention_mask,

  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
      File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
encoder_attention_mask,
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
            return CheckpointFunction.apply(function, preserve, *args)    return CheckpointFunction.apply(function, preserve, *args)            return CheckpointFunction.apply(function, preserve, *args)    
return CheckpointFunction.apply(function, preserve, *args)
return CheckpointFunction.apply(function, preserve, *args)return CheckpointFunction.apply(function, preserve, *args)return CheckpointFunction.apply(function, preserve, *args)
return CheckpointFunction.apply(function, preserve, *args)  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward

  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward



  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward

  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward
    outputs = run_function(*args)    
outputs = run_function(*args)  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward

          File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward
            outputs = run_function(*args)    outputs = run_function(*args)outputs = run_function(*args)outputs = run_function(*args)outputs = run_function(*args)
outputs = run_function(*args)

  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward


  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward

  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward
    return module(*inputs, use_cache, output_attentions)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return module(*inputs, use_cache, output_attentions)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return module(*inputs, use_cache, output_attentions)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return module(*inputs, use_cache, output_attentions)        
return module(*inputs, use_cache, output_attentions)return module(*inputs, use_cache, output_attentions)  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl


          File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
return module(*inputs, use_cache, output_attentions)return module(*inputs, use_cache, output_attentions)

  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward
    return forward_call(*input, **kwargs)    
return forward_call(*input, **kwargs)      File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward

output_attentions=output_attentions,  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward

  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward
    return forward_call(*input, **kwargs)    
output_attentions=output_attentions,
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    output_attentions=output_attentions,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    output_attentions=output_attentions,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    output_attentions=output_attentions,    
output_attentions=output_attentions,  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl

  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    output_attentions=output_attentions,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    output_attentions=output_attentions,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
        return forward_call(*input, **kwargs)    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward

      File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
      File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn
return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
      File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn
attn_weights = nn.functional.softmax(attn_weights, dim=-1)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)    attn_weights = nn.functional.softmax(attn_weights, dim=-1)attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)

  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn

          File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)attn_weights = nn.functional.softmax(attn_weights, dim=-1)attn_weights = nn.functional.softmax(attn_weights, dim=-1)


  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
    attn_weights = nn.functional.softmax(attn_weights, dim=-1)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
        attn_weights = nn.functional.softmax(attn_weights, dim=-1)attn_weights = nn.functional.softmax(attn_weights, dim=-1)

  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
    attn_weights = nn.functional.softmax(attn_weights, dim=-1)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
Traceback (most recent call last):
  File "codeparrot_training.py", line 246, in <module>
    loss = model(batch, labels=batch, use_cache=False).loss
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
        ret = input.softmax(dim)    ret = input.softmax(dim)    
    ret = input.softmax(dim)    
    ret = input.softmax(dim)ret = input.softmax(dim)
    ret = input.softmax(dim)RuntimeErrorret = input.softmax(dim)

RuntimeErrorret = input.softmax(dim)
: RuntimeError
: 
RuntimeErrorCUDA out of memory. Tried to allocate 160.00 MiB (GPU 7; 39.59 GiB total capacity; 15.60 GiB already allocated; 35.19 MiB free; 16.12 GiB reserved in total by PyTorch)    : RuntimeErrorCUDA out of memory. Tried to allocate 160.00 MiB (GPU 1; 39.59 GiB total capacity; 15.59 GiB already allocated; 43.19 MiB free; 16.11 GiB reserved in total by PyTorch): 
RuntimeErrorreturn forward_call(*input, **kwargs)CUDA out of memory. Tried to allocate 160.00 MiB (GPU 3; 39.59 GiB total capacity; 15.60 GiB already allocated; 35.19 MiB free; 16.12 GiB reserved in total by PyTorch): RuntimeError
CUDA out of memory. Tried to allocate 160.00 MiB (GPU 4; 39.59 GiB total capacity; 15.60 GiB already allocated; 35.19 MiB free; 16.12 GiB reserved in total by PyTorch): RuntimeError

CUDA out of memory. Tried to allocate 160.00 MiB (GPU 5; 39.59 GiB total capacity; 15.60 GiB already allocated; 35.19 MiB free; 16.12 GiB reserved in total by PyTorch): 
CUDA out of memory. Tried to allocate 160.00 MiB (GPU 8; 39.59 GiB total capacity; 15.60 GiB already allocated; 35.19 MiB free; 16.12 GiB reserved in total by PyTorch):   File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward

CUDA out of memory. Tried to allocate 160.00 MiB (GPU 2; 39.59 GiB total capacity; 15.60 GiB already allocated; 35.19 MiB free; 16.12 GiB reserved in total by PyTorch)
CUDA out of memory. Tried to allocate 160.00 MiB (GPU 6; 39.59 GiB total capacity; 15.60 GiB already allocated; 35.19 MiB free; 16.12 GiB reserved in total by PyTorch)

Traceback (most recent call last):
  File "codeparrot_training.py", line 246, in <module>
    return_dict=return_dict,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward
    loss = model(batch, labels=batch, use_cache=False).loss
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    encoder_attention_mask,
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward
    outputs = run_function(*args)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    return module(*inputs, use_cache, output_attentions)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward
    output_attentions=output_attentions,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    return_dict=return_dict,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn
    attn_weights = nn.functional.softmax(attn_weights, dim=-1)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward
        ret = input.softmax(dim)encoder_attention_mask,

  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
RuntimeError: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 10; 39.59 GiB total capacity; 15.60 GiB already allocated; 35.19 MiB free; 16.12 GiB reserved in total by PyTorch)
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward
    outputs = run_function(*args)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward
    return module(*inputs, use_cache, output_attentions)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward
    output_attentions=output_attentions,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn
    attn_weights = nn.functional.softmax(attn_weights, dim=-1)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
    ret = input.softmax(dim)
RuntimeError: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 11; 39.59 GiB total capacity; 15.60 GiB already allocated; 35.19 MiB free; 16.12 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "codeparrot_training.py", line 246, in <module>
Traceback (most recent call last):
  File "codeparrot_training.py", line 246, in <module>
Traceback (most recent call last):
  File "codeparrot_training.py", line 246, in <module>
    loss = model(batch, labels=batch, use_cache=False).loss
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    loss = model(batch, labels=batch, use_cache=False).loss
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    loss = model(batch, labels=batch, use_cache=False).loss
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
Traceback (most recent call last):
  File "codeparrot_training.py", line 246, in <module>
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
    loss = model(batch, labels=batch, use_cache=False).loss
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward
    return_dict=return_dict,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return_dict=return_dict,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return_dict=return_dict,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward
    encoder_attention_mask,
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
    encoder_attention_mask,
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward
    return_dict=return_dict,    
      File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
return CheckpointFunction.apply(function, preserve, *args)outputs = run_function(*args)

  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward
    outputs = run_function(*args)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward
    encoder_attention_mask,
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward
        return module(*inputs, use_cache, output_attentions)outputs = run_function(*args)

  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward
    return module(*inputs, use_cache, output_attentions)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    encoder_attention_mask,
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward
    return module(*inputs, use_cache, output_attentions)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward
    output_attentions=output_attentions,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    outputs = run_function(*args)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward
    output_attentions=output_attentions,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    return module(*inputs, use_cache, output_attentions)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)    
output_attentions=output_attentions,  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward

  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn
    attn_weights = nn.functional.softmax(attn_weights, dim=-1)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
    attn_weights = nn.functional.softmax(attn_weights, dim=-1)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    output_attentions=output_attentions,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn
    attn_weights = nn.functional.softmax(attn_weights, dim=-1)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
    ret = input.softmax(dim)
RuntimeError:     CUDA out of memory. Tried to allocate 160.00 MiB (GPU 14; 39.59 GiB total capacity; 15.60 GiB already allocated; 35.19 MiB free; 16.12 GiB reserved in total by PyTorch)ret = input.softmax(dim)

    RuntimeErrorreturn forward_call(*input, **kwargs): 
CUDA out of memory. Tried to allocate 160.00 MiB (GPU 13; 39.59 GiB total capacity; 15.60 GiB already allocated; 35.19 MiB free; 16.12 GiB reserved in total by PyTorch)  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward

    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn
    attn_weights = nn.functional.softmax(attn_weights, dim=-1)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
    ret = input.softmax(dim)
RuntimeError: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 9; 39.59 GiB total capacity; 15.60 GiB already allocated; 35.19 MiB free; 16.12 GiB reserved in total by PyTorch)
    ret = input.softmax(dim)
RuntimeError: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 12; 39.59 GiB total capacity; 15.60 GiB already allocated; 35.19 MiB free; 16.12 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "codeparrot_training.py", line 246, in <module>
    loss = model(batch, labels=batch, use_cache=False).loss
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/accelerate/utils.py", line 296, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 141, in decorate_autocast
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1057, in forward
    return_dict=return_dict,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 884, in forward
    encoder_attention_mask,
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 211, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py", line 90, in forward
    outputs = run_function(*args)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 873, in custom_forward
    return module(*inputs, use_cache, output_attentions)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 401, in forward
    output_attentions=output_attentions,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/oskar_bonde_ericsson_com/.local/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 212, in _attn
    attn_weights = nn.functional.softmax(attn_weights, dim=-1)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py", line 1679, in softmax
    ret = input.softmax(dim)
RuntimeError: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 15; 39.59 GiB total capacity; 15.93 GiB already allocated; 3.19 MiB free; 16.43 GiB reserved in total by PyTorch)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 6835) of binary: /opt/conda/bin/python3.7
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
  role_ranks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
  global_ranks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
  role_world_sizes=[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]
  global_world_sizes=[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/2/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/3/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker4 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/4/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker5 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/5/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker6 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/6/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker7 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/7/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker8 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/8/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker9 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/9/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker10 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/10/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker11 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/11/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker12 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/12/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker13 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/13/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker14 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/14/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker15 reply file to: /tmp/torchelastic_bmnbki_c/none_087jy0na/attempt_1/15/error.json
